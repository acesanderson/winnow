{#
    Template for pairwise comparison as described in the Themis LLM-as-a-Judge paper
    
    Variables:
    - scenario_name: The name of the specific scenario being evaluated (e.g., "Close QA", "Open QA", "Math-related QA", etc.)
    - scenario_description: Brief description explaining what the scenario entails
    - criteria: The judge criteria specific to the scenario, should contain numbered evaluation points in order of importance
    - instruction: The original user instruction/query being evaluated
    - response_1: The first AI assistant's response to be compared
    - response_2: The second AI assistant's response to be compared
    
    This template implements the pairwise comparison approach from the paper,
    allowing for direct comparison of two responses to the same query.
    It maintains the same evaluation criteria and five-tier grading system
    while adding a comparative judgment (better/worse/tied).
#}

Your task is to evaluate the quality of AI responses. You are well aware that when a user issues an instruction of {scenario_name} (the definition is: {scenario_description}), an AI assistant's response should meet the following criteria (listed in descending order of importance):

<criteria>
{criteria}
</criteria>

The grading uses a five-tier system (1–5), the meanings of each tier are:

<grading_tiers>
1 The response has significant flaws, totally deviates from the criteria, and should not be seen in practice.
2 The response has parts that meet the criteria and can be adopted, but as a whole, the quality is not sufficient.
3 The response has a mix of strengths and weaknesses, with strengths overall outweighing the weaknesses within the evaluation criteria.
4 The response is of acceptable quality, overall meets the criteria, and has few minor issues that can be improved. When a reference answer is given, this tier represents the quality shown by the reference answer.
5 The response is excellent, strictly meets the criteria in all aspects. When a ref answer is given, this tier represents a quality superior to the answer.
</grading_tiers>

Regarding a user instruction of {scenario_name}, we have collected two responses from AI assistants. Based on your understanding of the current standards for responses, comprehensively evaluate and determine which response is better or if they are tied (including both being good or both being bad). Below are the user instruction and the assistant's response data:

<data>
<user_instruction>
{instruction}
</user_instruction>
<response_1>
{response_1}
</response_1>
<response_2>
{response_2}
</response_2>
</data>

You need to follow these steps to evaluate the above response:
1. Recall the relevant AI assistant response criteria and carefully read and understand the response to be evaluated.
2. Identify from all criteria the key ones for the current user instruction and response, including those that performed well and those that did not.
3. Besides the given criteria, add any other important criteria that you think are necessary for evaluating the current user instruction response.
4. Based on your final selection of criteria, assign scores (1–5) to each criterion, and provide an overall score by weighting all sub-scores.

Think carefully and then provide your conclusion. Your response should keep the '[[' and ']]' symbols in the output:
I believe [[Response 1 is better]]/[[Response 2 is better]]/[[Both Responses are tied]], with the overall score for Response 1 being [[a score between 1 and 5]], and the overall score for Response 2 being [[a score between 1 and 5]], based on the following reasons:
1. (Please detail your reasons in order of importance from high to low, each standard also attaching the [[scores]] for both responses under that standard...)
